{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Embedding for Clustering\n",
    "\n",
    "In this tutorial we will implement deep embedding for clustering, an end-to-end neural network solution for clustering using an autoencoder approach for network pretrainin. While the MNIST dataset is used, this implementation uses a multilayer perceptron architecture and may be generalized to any non-imaging neural network task.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1511.06335.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras import Input, Model, layers, losses, optimizers, datasets\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following code block will prepare the MNIST dataset as archived by the Tensorflow / Keras library. For purposes of demonstration, both the train and valid cohorts are combined into a single array. Additionally, the original data to type `uint8` on range `[0, 255]` is scaled to range of `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    \n",
    "    # --- Combine train and valid for demo purposes\n",
    "    x = np.concatenate((x_train, x_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    \n",
    "    # --- Flatten and scale\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = x / 255.\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data\n",
    "X, y = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-dimensional latent space visualization\n",
    "\n",
    "At various points during this tutorial, we will examine the quality of the (low-dimensional) latent space embedding in separating out our ground-truth MNIST dataset. To do so, we will use the following code block which performs a 2-dimensional PCA and plots the results with each digit encoded in a different color.\n",
    "\n",
    "Note that **any** valid representation of `X` may be provided into this function, including the original 784-element raw data (acknowledging that the quality of this embedding is of course limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, y):\n",
    "    \"\"\"\n",
    "    Method to show PCA reduced features X\n",
    "    \n",
    "    :params\n",
    "    \n",
    "      (np.ndarray) X : 2D feature representation of size (rows, columns)\n",
    "      (np.ndarray) y : 1D vector with ground-truth MNIST labels of size (rows,)\n",
    "    \n",
    "    \"\"\"\n",
    "    pyplot.clf()\n",
    "\n",
    "    # --- Create PCA\n",
    "    p = PCA(n_components=2)\n",
    "    c = p.fit_transform(X)\n",
    "\n",
    "    # --- Scatter\n",
    "    fig = pyplot.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for i in range(10):\n",
    "        ax.scatter(c[y == i, 0], c[y == i, 1], s=2, marker='.', alpha=0.5)\n",
    "\n",
    "    ax.axis('off')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "In addition to viewing the quality of low-dimensional latent space embeddings, we will quantitatively evaluate the embedding in terms of separating out the ground-truth MNIST digits using a k-means clustering algorithm. The derived clusters are compared against ground-truth using an adjusted rand score (for more details see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_kmeans(X, y):\n",
    "    \n",
    "    y_ = KMeans(n_clusters=10).fit_predict(X)\n",
    "    print('Adjusted rand score: {:0.4f}'.format(adjusted_rand_score(y_, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by building a simple 2 hidden layer MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base(shape=(784,)):\n",
    "    \n",
    "    x = Input(shape=shape)\n",
    "    \n",
    "    l0 = layers.Dense(256, activation='relu')(x)\n",
    "    l1 = layers.Dense(256, activation='relu')(l0)\n",
    "    \n",
    "    embedding = layers.Dense(10)(l1)\n",
    "    \n",
    "    return Model(inputs=x, outputs=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further modification (e.g., after initialization with random weights), the quality of the feature embedding yielded by this MLP is poor. You can confirm this by creating this model, running a forward pass through the MNIST dataset, and evaluating the output feature representation using the `pca(...)` and `assess_kmeans(...)` methods above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create baseline (untrained) model\n",
    "base = create_base()\n",
    "y_ = base.predict(X)\n",
    "\n",
    "# --- Test baseline (untrained) model representation\n",
    "pca(y_, y)\n",
    "assess_kmeans(y_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "Let us improve the quality of this feature representation using an autoencoder. The following simple architecture is symmetric to the original baseline network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(base, shape=(784,)):\n",
    "    \n",
    "    x = Input(shape=shape)\n",
    "    \n",
    "    l0 = layers.Dense(256, activation='relu')(base(x))\n",
    "    l1 = layers.Dense(256, activation='relu')(l0)\n",
    "    \n",
    "    recon = layers.Dense(784)(l1)\n",
    "    \n",
    "    return Model(inputs=x, outputs=recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to attach this autoencoder to the original base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create autoencoder\n",
    "base = create_base()\n",
    "autoencoder = create_autoencoder(base=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, we will use standard baseline hyperparameters including an Adam optimizer with mean squared error reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compile and train\n",
    "autoencoder.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "autoencoder.fit(x=X, y=X, batch_size=100, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us re-test the quality of our model latent space representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run a forward pass of data through trained encoder\n",
    "y_ = base.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test baseline (trained) model representation\n",
    "pca(y_, y)\n",
    "assess_kmeans(y_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend the current model into a deep clustering network, we need to create one additional special layer that will be appended to the end of our base network e.g., a *clustering* layer. Assuming a total of *k* clusters, the **weights** of this clustering layer represent the centers of the *k* different centroids. Additionally, the **output** of this clustering layer represents a probability distribution of cluster assignments e.g., the likelihood of any given single input to belong in any of *k* different clusters. More specifically, this probability distribution is modeled by a student's t-distribution as shown in the code below.\n",
    "\n",
    "Implementation of a Keras clustering layer taken from: https://github.com/XifengGuo/DEC-keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    :params\n",
    "    \n",
    "      n_clusters : number of clusters.\n",
    "      weights    : list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "      alpha      : parameter in Student's t-distribution. Default to 1.0.\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        \n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "            \n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = layers.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        \n",
    "        self.input_spec = layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        \n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" \n",
    "        Method to calcualte student t-distribution (same as used in t-SNE algorithm)\n",
    "\n",
    "        :params\n",
    "        \n",
    "          inputs : the variable containing data, shape=(n_samples, n_features)\n",
    "        \n",
    "        :return\n",
    "        \n",
    "            q    : student's t-distribution, or soft labels for each sample of shape (n_samples, n_clusters)\n",
    "            \n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        \n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        \n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        \n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        \n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, use the following cell block to append a clustering layer (with 10 different clusters) to the base network pretrained from the autoencoder task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build deep clustering model\n",
    "x = Input(shape=(784,))\n",
    "clustering_layer = ClusteringLayer(n_clusters=10, name='clustering')(base(x))\n",
    "model = Model(inputs=x, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, the **weights** of the clustering layer represent the cluster centroid centers. While our model will learn the optimal clusters over time, we will initiate the weight values here using the results of a naive k-means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run k-means on the output of the base network\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(base.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set weights of clustering layer\n",
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training objective**: \n",
    "\n",
    "In each training step, our deep clustering model will attempt to update two objectives simultaneously:\n",
    "\n",
    "1. Improve the latent space feature representation of our base network to more closely align each training example with its closest cluster center (e.g., update to the base network weights).\n",
    "\n",
    "2. Improve the cluster centers to better separate the training data into different groups (e.g., update to the clustering layer weights).\n",
    "\n",
    "To train this objective, the output of the clustering layer (e.g., the predicted probability distribution) is optimized against an updated target probability distribution as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This target distribution is derived in two steps:\n",
    "\n",
    "1. Multiply the output distribution by itself (`q ** 2`): this acts to strengthen the high-probability predictions and decrease the low-probability predictions (e.g., predictions above 0.5 go towards 1.0 and those below 0.5 go towards 0.0).\n",
    "\n",
    "2. Normalize the predictions against each cluster class: this acts to keep a relatively balanced distribution across all clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we are set to train the deep clustering model. To do so, we will use the following steps:\n",
    "\n",
    "1. Create a target distribution `p` based on the cluster layer output `q`.\n",
    "2. Training the model for 1 epoch to based on the target distribution.\n",
    "3. Visualize the new latent feature representation.\n",
    "4. Repeat the loop by creating a new target distribution as in step (1).\n",
    "\n",
    "Use the following cell to run model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='kld')\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    q = model.predict(X)\n",
    "    p = target_distribution(q)\n",
    "    \n",
    "    print('Rand score: {:0.4f}'.format(adjusted_rand_score(q.argmax(axis=1), y)))\n",
    "    pca(base.predict(X), y)\n",
    "    \n",
    "    model.fit(x=X, y=p, batch_size=100, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us re-test the quality of our model latent space representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run a forward pass of data through deep clustering model\n",
    "y_ = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test baseline (trained) model representation\n",
    "pca(y_, y)\n",
    "assess_kmeans(y_, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
