{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Bayesian Networks for Active Learning\n",
    "\n",
    "In this tutorial we will implement active learning through Deep Bayesian networks. While the MNIST dataset is used, this implementation uses a multilayer perceptron architecture and may be generalized to any non-imaging neural network task.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1703.02910.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tensorflow.keras import Input, Model, layers, losses, optimizers, datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following code block will prepare the MNIST dataset as archived by the Tensorflow / Keras library. For purposes of demonstration, both the train and valid cohorts are combined into a single array. Additionally, the original data to type `uint8` on range `[0, 255]` is scaled to range of `[0, 1]`.\n",
    "\n",
    "In an active learning task, one goal is to ensure that the rare (out-of-distribution) examples are prioritized for training. To simulate this, we will arbitrarily limit the number of training examples for a designated minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(minority_class=9, minority_class_count=1000):\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "    # --- Combine train and valid for demo purposes\n",
    "    x = np.concatenate((x_train, x_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "\n",
    "    # --- Flatten and scale\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = x / 255.\n",
    "\n",
    "    # --- Artificially reduce examples of minority class\n",
    "    i = np.nonzero(y == minority_class)[0][minority_class_count:]\n",
    "    x = np.delete(x, i, axis=0)\n",
    "    y = np.delete(y, i, axis=0)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to standard neural network, a deep Bayesian model learns each network parameter as a distribution of values. To evaluate a deep Bayesian model on any given single input, an integration must be performed over all possible model parameters (e.g., an ensemble of infinite neural networks). Using this approach, the **uncertainty** of any given prediction may be estimated (e.g., how much variation exists from prediction to prediction).\n",
    "\n",
    "As an approximate solution to this problem, a deep Bayesian model may be estimated using a Monte Carlo simulation using random **dropout** applied during inference. \n",
    "\n",
    "Let us start by building a simple 2 hidden layer MLP. Note that in this implementation, the `training` flag is set to `None` by default to indicate the standard dropout behavior e.g., dropout is applied during training but removed during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(shape=(784,), training=None):\n",
    "\n",
    "    x = Input(shape=shape)\n",
    "\n",
    "    # --- Layer 1\n",
    "    l0 = layers.Dense(256, activation='relu')(x)\n",
    "    d0 = layers.Dropout(0.5)(l0, training=training)\n",
    "    \n",
    "    # --- Layer 2\n",
    "    l1 = layers.Dense(256, activation='relu')(d0)\n",
    "    d1 = layers.Dropout(0.5)(l1, training=training)\n",
    "    \n",
    "    # --- Logits\n",
    "    l2 = layers.Dense(10)(d1)\n",
    "\n",
    "    # --- Create model\n",
    "    model = Model(inputs=x, outputs=l2)\n",
    "\n",
    "    # --- Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, dropout is not applied during inference. To create a new \"shadow\" model identical in network architecture but with dropout manually activated (e.g., `training=True`), use the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dropout(model):\n",
    "    \"\"\"\n",
    "    Method to re-create model with dropout manually activated \n",
    "\n",
    "    \"\"\"\n",
    "    model_dropout = create_model(training=True) \n",
    "    model_dropout.set_weights(model.get_weights())\n",
    "\n",
    "    return model_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During model training in this demonstration, we will alternative between training and evaluating the standard dropout behavior while simultaneously performing active learning using the dropout activated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning\n",
    "\n",
    "Provided a large pool of unlabeled data, active learning strategies help to identify the out-of-distribution examples that may yield the greatest incremental gain in model performance. Such strategies may be valuable in supervised deep learning for healthcare as resources for annotating medical data are often scarce.\n",
    "\n",
    "The deep Bayesian network implementation of active learning identifies out-of-distribution examples using various estimates of prediction **uncertainty**. Given a total of `n` repeated predictions (e.g., forward passes with dropout enabled), uncertainty can be estimated by measuring the degree of variation between predictions for the same training example. Formal metrics for measuring prediction variation are known as *acquisition functions* and include:\n",
    "\n",
    "* entropy\n",
    "* variation ratio\n",
    "* BALD metric (Bayesian active learning by disagreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking strategies\n",
    "\n",
    "To implement these various ranking strategies (acquisition functions), we will use the following Python decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(func):\n",
    "    \"\"\"\n",
    "    Decorator for sampling function\n",
    "\n",
    "      (1) Create subcohort to feed into acquisition function (for speed)\n",
    "      (2) Run repeated forward passes of model (with dropout enabled)\n",
    "      (3) Find top n_samples of most informative data using acquisition function\n",
    "      (4) Remove top n_samples from training pool\n",
    "\n",
    "    \"\"\"\n",
    "    def wrapper(model, x, y, n_samples=10, repeats=20, subcohort=2000, *args, **kwargs):\n",
    "\n",
    "        # --- Create subcohort for speed \n",
    "        i_sub = np.random.permutation(x.shape[0])[:subcohort]\n",
    "        x_sub = x[i_sub]\n",
    "        y_sub = y[i_sub]\n",
    "\n",
    "        # --- Run repeated forward passes\n",
    "        if hasattr(model, 'predict'):\n",
    "            m = create_model_dropout(model)\n",
    "            preds = [m.predict(x_sub) for _ in range(repeats)]\n",
    "            preds = np.array([tf.nn.softmax(p, axis=-1) for p in preds])\n",
    "\n",
    "        else:\n",
    "            preds = None\n",
    "\n",
    "        indices = func(preds=preds, x=x_sub, y=y_sub, n_samples=n_samples, *args, **kwargs)\n",
    "        top_n = i_sub[indices[:n_samples]]\n",
    "\n",
    "        x_new = x[top_n]\n",
    "        y_new = y[top_n]\n",
    "\n",
    "        x = np.delete(x, top_n, axis=0)\n",
    "        y = np.delete(y, top_n, axis=0)\n",
    "\n",
    "        return x_new, y_new, x, y\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this wrapper, we will define a number of acquisition functions (passed as the generic `func` object above). All functions will be defined using the following API:\n",
    "\n",
    "```python\n",
    "@sample_data\n",
    "def func(preds, **kwargs):\n",
    "    \n",
    "    # --- Method to create indices ranking various predictions\n",
    "    indices = ...\n",
    "    \n",
    "    return indices\n",
    "```\n",
    "\n",
    "The decorator function will be responsible for the remaining implementation steps, including sampling a total number of `repeats` predictions from the `model` object as well as recreating the training cohort based on the top ranked examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Many of the acquisition functions rely on a calculation of the entropy associated with a probability distribution.\n",
    "\n",
    "![Entropy](https://i0.wp.com/jeanvitor.com/wp-content/uploads/2017/11/entropyequation.png)\n",
    "\n",
    "The following function will be used to calculate entropy generically for various ranking strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probs, epsilon=1e-10):\n",
    "\n",
    "    return -np.sum(probs * np.log(probs + epsilon), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial sampling\n",
    "\n",
    "To initialize training for all experiments, a baseline distribution of data will be sampled in a balanced manner across all label classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sample_data\n",
    "def rank_initial(y, n_samples, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create initial balanced uniform sampling from each class \n",
    "\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "\n",
    "    uniques = np.unique(y)\n",
    "    n = int(n_samples / uniques.size)\n",
    "\n",
    "    for value in uniques:\n",
    "        i = np.nonzero(y == value)[0]\n",
    "        indices.append(i[np.random.permutation(i.size)[:n]])\n",
    "\n",
    "    return np.array(indices).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling\n",
    "\n",
    "As a baseline strategy for learning, data will be added incrementally to the training cohort through random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sample_data\n",
    "def rank_random(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement random sampling strategy\n",
    "\n",
    "    \"\"\"\n",
    "    return np.random.permutation(x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy-based sampling\n",
    "\n",
    "The following code block implements a sampling strategy based on the overall entropy of the model predictions after first aggregating all predictions by taking the mean across all repeated samples. Note that compared to the *BALD* metric strategy below, this strategy evaluates the *aggregate* entropy of all repeated samples and does not take into consideration the degree of potential variation that may exist between individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sample_data\n",
    "def rank_entropy(preds, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement acquisition ranking via entropy\n",
    "\n",
    "    :params\n",
    "\n",
    "      (np.ndarray) preds : prediction array of shape (repeats, batch, classes)\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Find mean prob distribution across all repeated samples\n",
    "    preds = np.mean(preds, axis=0)\n",
    "\n",
    "    # --- Calculate entropy across mean prob distribution\n",
    "    entropy = calculate_entropy(preds)\n",
    "\n",
    "    return np.argsort(-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation ratio sampling\n",
    "\n",
    "In this strategy, the `mode` (e.g., most common) *count* of all repeated predictions for a given example is calculated and used as an approximate estimate to the degree of prediction variance. For a total of `repeats` number of predictions, if all predictions are identical, then the *count* of the prediction `mode` will be identical to `repeats` and the degree of variation will be `0`. If all predictions are completely different, then the *count* of the prediction `mode` will be `1` and the degree of variation will be maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sample_data\n",
    "def rank_var_ratio(preds, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement acquisition ranking via variation ratio\n",
    "\n",
    "    :params\n",
    "\n",
    "      (np.ndarray) preds : prediction array of shape (repeats, batch, classes)\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Convert probs to label classes\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # --- Determine how often predictions agree\n",
    "    mode, count = stats.mode(preds, axis=0)\n",
    "\n",
    "    var = 1 - count.ravel() / preds.shape[0]\n",
    "\n",
    "    return np.argsort(-var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALD sampling\n",
    "\n",
    "The Bayesian active learning by disagreement (BALD) strategy is similar to a ranking based on entropy (above). However instead of evaluating the entropy of the mean aggregate of all `repeats` number of predictions, the BALD strategy calculates the **marginal** entropy of all *individual* `repeats` number of predictions relative to the baseline mean aggregation. In this way, the degree of variation between each repeated prediction is assessed instead of the baseline degree of entropy itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sample_data\n",
    "def rank_bald(preds, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to implement acquisition ranking via BALD \n",
    "\n",
    "    :params\n",
    "\n",
    "      (np.ndarray) preds : prediction array of shape (repeats, batch, classes)\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Calculate entropy across each repeated prob distribution\n",
    "    entropy_all = np.mean(calculate_entropy(preds), axis=0)\n",
    "\n",
    "    # --- Calculate entropy across the mean of repeated prob distributions\n",
    "    preds = np.mean(preds, axis=0)\n",
    "    entropy_rpt = calculate_entropy(preds)\n",
    "\n",
    "    bald = entropy_rpt - entropy_all\n",
    "\n",
    "    return np.argsort(-bald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The following code blocks are used to generically run an experiment using a specified acquisition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "To evaluate model performance, we will use the following function to evaluate accuracy on the remaining data yet to be used for algorithm training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x, y):\n",
    "    \"\"\"\n",
    "    Method to test model on remaining data\n",
    "\n",
    "    \"\"\"\n",
    "    y_ = model.predict(x)\n",
    "    y_ = np.argmax(y_, axis=-1)\n",
    "\n",
    "    return (y_ == y).sum() / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will evaluate training performance on both the entire and minority class individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(model, x, y, size, minority_class=9, **kwargs):\n",
    "\n",
    "    acc_all = test_model(model, x, y)\n",
    "    acc_min = test_model(model, x[y == minority_class], y[y == minority_class])\n",
    "\n",
    "    print('Model performance ({} samples): {:0.5f} (all) | {:0.5f} (minority)'.format(\n",
    "        str(size).rjust(4), acc_all, acc_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train each model, we will specify:\n",
    "\n",
    "* `acquisition_func`: the specific ranking strategy above\n",
    "* `rounds`: total number of training rounds\n",
    "* `n_samples`: total number of training examples to add each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, acquisition_func, rounds=10, n_samples=100, batch_size=128, epochs=50, **kwargs):\n",
    "\n",
    "    # --- Create initial uniform sampling of 20 training examples\n",
    "    x, y = load_mnist()\n",
    "    x_train, y_train, x, y = rank_initial(model=model, x=x, y=y, n_samples=20, **kwargs)\n",
    "\n",
    "    # --- Create initial fit\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "    print_performance(model, x, y, y_train.size, **kwargs)\n",
    "\n",
    "    for n in range(rounds):\n",
    "\n",
    "        # --- Get additional data\n",
    "        x_new, y_new, x, y = acquisition_func(model=model, x=x, y=y, n_samples=n_samples, **kwargs)\n",
    "        x_train = np.concatenate((x_train, x_new))\n",
    "        y_train = np.concatenate((y_train, y_new))\n",
    "\n",
    "        # --- Train \n",
    "        model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=False)\n",
    "        print_performance(model, x, y, y_train.size, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Use the following cells to run each individual experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train using random sampling (baseline)\n",
    "model = create_model()\n",
    "train(model=model, acquisition_func=rank_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train using entropy\n",
    "model = create_model()\n",
    "train(model=model, acquisition_func=rank_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train using variation ratio\n",
    "model = create_model()\n",
    "train(model=model, acquisition_func=rank_var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train using BALD\n",
    "model = create_model()\n",
    "train(model=model, acquisition_func=rank_bald)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
