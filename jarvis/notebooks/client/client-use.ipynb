{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jarvis Data `Client()`\n",
    "\n",
    "The `Client()` object as part of the `jarvis.train.client` module provides a simple yet powerful interface for loading and preprocessing data in the context of neural network training. In this notebook we will explore the following functionality:\n",
    "\n",
    "* creating `Client()` objects and generators\n",
    "* customizing `Client()` via `*.yml` files\n",
    "* customizing `Client()` via class overload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "Jarvis is a custom Python package to facilitate data science and deep learning for healthcare. Let us begin by installing the Jarvis library and required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.general import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of a small toy cohort of non-contrast CT images of the head and corresponding whole brain segmentation masks.\n",
    "\n",
    "### Download\n",
    "\n",
    "The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/ct_bet_demo`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download data\n",
    "paths = datasets.download(name='ct/bet-demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the location of downloaded data is not required for baseline functionality, manipulation of the underlying data structures and configuration files may be needed for implementing customized training. As a result it may be useful to note this location for future reference. All code and key configuration files are located relative to a project `code` root directory. This directory can be retrieved in one of two methods:\n",
    "\n",
    "1. The return of a `datasets.download(...)` call with always yield a Python dictionary with two entries:\n",
    "\n",
    "* `paths['code']`: root directory of all Python code and configuration files\n",
    "* `paths['data']`: root directory of all raw data\n",
    "\n",
    "Note that by default, these two directories are identical and placed in `/data/raw/[datatset_name]`.\n",
    "\n",
    "2. Alternatively, the paths can be recovered using the `jarvis.utils.general.tools` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieve paths using Jarvis tools\n",
    "from jarvis.utils.general import tools as jtools\n",
    "\n",
    "paths = jtools.get_paths('ct/bet-demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the `paths['code']` directory, a standard folder hierachy can be typically found:\n",
    "\n",
    "```\n",
    "| --- data /\n",
    "      | --- csvs /\n",
    "          | --- db-all.csv.gz\n",
    "          ...\n",
    "      | --- ymls /\n",
    "          | --- db-all.yml\n",
    "          | --- client.yml\n",
    "          ...\n",
    "```\n",
    "\n",
    "Keep a note of this structure and reference this figure as needed in the rest of this tutorial. As we will be manipulating the `client.yml` file throughout the course of this tutorial, let us save a reference to this location now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set CLIENT_PATH for tutorial\n",
    "CLIENT_PATH = '{}/data/ymls/client.yml'.format(paths['code'])\n",
    "assert os.path.exists(CLIENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The primary function of the Jarvis data `Client()` is to provide a seamless interface for loading heterogenous, multidimensional healthcare datasets for machine learning training. The key consideration is that for complex datasets, the *method* by which one chooses to load data (single-slice, multiple slices, 3D, patch-based, etc) is in fact a key hyperparameter that will need to be tested empirically for best results. In addition, various permuations in data normalization, augmentation and prediction targets (binary vs. multiclass vs. multiple loss functons, etc) may need to be optimized during experimentation.\n",
    "\n",
    "The Jarvis `Client()` aims to consolidate these various options into a simple API that can be customized via a single `configs` Python dictionary or through a client `YAML` file. After defining the required parameters, an instantiatied `Client()` object can be used to then create training and validation Python generators that can be used agnostically in a number of machine learning libraries including Tensorflow / Keras.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Client()` object\n",
    "\n",
    "There are two ways of creating a `Client()` object. The first and simplest method is to use the `datasets.prepare(name...=, keyword=...)` function. The `name` argument references the dataset name (identical to the name used for data download). The `keyword` argument (optional) can be used to specify a particular predefined client configuration (otherwise the first `client*.yml` file in the `/data/ymls/` directory will be chosen). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating client / generators - option (1)\n",
    "gen_train, gen_valid, client = datasets.prepare('ct/bet-demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is convenient, and in fact creates the required Python generators needed for algorithm training, however the user will have somewhat limited control of the `Client()` object itself. For greater flexibility, consider a two-step approach whereby the client is first manually defined by pointing to a specific `*.yml` file, and then creating the required Python generators in a second step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create client / generators - option (2)\n",
    "client = Client(CLIENT_PATH)\n",
    "gen_train, gen_valid = client.create_generators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python generators\n",
    "\n",
    "At this point, the generic Python generators above will yield training and validation data that can be used to plug into standard machine learning library APIs. In Tensorflow / Keras, use the `model.fit(...)` function:\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    x=gen_train,\n",
    "    validation_data=gen_valid\n",
    "    ...)\n",
    "```\n",
    "\n",
    "Note that any number of valid Python iterator techniques can be used to access the data manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load next training batch\n",
    "xs, ys = next(gen_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `xs` and `ys` \n",
    "\n",
    "Each iteration through the `Client()` created Python generator yields two dictionaries, `xs` and `ys`, which conform to the Tensorflow / Keras API for model input(s) and output(s). This generic approach of yielding Python dictionary variables allows for flexible definition of one or more input(s) and output(s), as long as the same naming conventions are used by the `Client()` Python generator and Tensorflow / Keras 2.0 `Model()`. This schematic will highlight the key considerations:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "\n",
    "# --- Define model\n",
    "\n",
    "Client()             Model()\n",
    "\n",
    "xs.keys()            inputs = {}\n",
    "* 'dat_1'       ==>  inputs['dat_1'] = Input(..., name='dat_1')\n",
    "* 'dat_2'       ==>  inputs['dat_2'] = Input(..., name='dat_2')\n",
    "* ...\n",
    "\n",
    "ys.keys()            logits = {}\n",
    "* 'lbl_1'       ==>  logits['lbl_1'] = layers.Conv3D(..., name='lbl_1')(...)\n",
    "* 'lbl_2'       ==>  logits['lbl_2'] = layers.Conv3D(..., name='lbl_2')(...)\n",
    "* ...\n",
    "\n",
    "# --- Instantiate model\n",
    "\n",
    "model = Model(inputs=inputs, logits=logits)\n",
    "\n",
    "# --- Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimzer=...,\n",
    "    loss={\n",
    "        'lbl_1': ..., \n",
    "        'lbl_2': ...},\n",
    "    metrics={\n",
    "        'lbl_1': ...,\n",
    "        'lbl_2': ...})\n",
    "\n",
    "```\n",
    "\n",
    "To summarize,\n",
    "\n",
    "1. All keys in `xs.keys()` must:\n",
    "\n",
    "* match a correponding key in the `inputs` dict that is eventually passed to `Model(inputs=inputs, ...)`\n",
    "* match a `name` of the Keras input tensor defined via `Input(..., name=name)`\n",
    "\n",
    "2. All keys in `ys.keys()` must:\n",
    "\n",
    "* match a correponding key in the `logits` dict that is eventually passed to `Model(..., logits=logits)`\n",
    "* match a `name` of the Keras output logit tensor defined via `layers.__(..., name=name)(...)`\n",
    "* match a corresponding key in the `loss` and `metrics` arguments during the `model.compile(...)` call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "For algorithm testing, it may be necessary to load data in a different manner than during training. For example, even if single 2D slices from a 3D volume are used during training (or smaller patches from a large single 2D image), it will be important to perform algorithm testing of the full volume. To create dedicated Python generators in *test mode* (full 3D matrix shape), pass the `test=True` argument into the `client.create_generators(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create test generators\n",
    "test_train, test_valid = client.create_generators(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in most scenarios, the `test_valid` generator will load the primary cohort needed for testing (e.g. validation split). The `test_train` generator can be used to iterate through training cohort itself if comparisons between train / valid performance are desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "The configuration details for a `Client()` object are defined in a client `*.yml` file (the same file that is manually based when to the `Client()` constructor when instantiating a new object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate a new Client\n",
    "client = Client(CLIENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the contents of the `client.yml` file in this example\n",
    "\n",
    "```yml\n",
    "_id:\n",
    "  project: ct/bet-demo\n",
    "  version: null\n",
    "_db: /data/ymls/db-sum.yml\n",
    "batch:\n",
    "  fold: -1\n",
    "  sampling:\n",
    "    bg: 0.5\n",
    "    fg: 0.5\n",
    "  size: 8\n",
    "specs:\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms:\n",
    "        clip:\n",
    "          max: 256\n",
    "          min: 0\n",
    "        scale: 64\n",
    "        shift: 32\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "    msk:\n",
    "      dtype: float32\n",
    "      loads: msk\n",
    "      norms:\n",
    "        clip:\n",
    "          max: 1\n",
    "          min: 0\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ys:\n",
    "    bet:\n",
    "      dtype: uint8\n",
    "      loads: bet\n",
    "      norms:\n",
    "        clip:\n",
    "          max: 1\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "```\n",
    "\n",
    "The `_id` and `_db` fields are protected attributes that should not be changed. For more information see corresponding tutorial for creating `Client()` objects. In this tutorial, we will focus on customizing the `batch` and `specs` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Client()` Data\n",
    "\n",
    "In general a total of `N`-number of training examples are defined for each cohort, where a *training example* is represented by the minimal single unit used for training (e.g. a single 2D slice, multi-slice slab, 2D patch, etc). Each unit of training is specified as a single row in a master `*.csv(.gz)` file (see folder hierarchy above). The data itself is stored as two attributes (Pandas `DataFrame`) within the `client.db` variable:\n",
    "\n",
    "* `client.db.fnames`: all relevant file names for serialized data\n",
    "* `client.db.header`: all other relevant data in addition to file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pandas DataFrame for serialized data file fnames\n",
    "client.db.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pandas DataFrame for all other non file name data\n",
    "client.db.header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that file names are often repeated in this representation as each **row** represents a single 2D slice; since many 2D slices are used a training (e.g. each row represents one *unit* of training), each file containing a full 3D volume will be repeated. To see the full combined data from any given single row, use the `client.db.row(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show specification for first row of data\n",
    "client.db.row(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the raw metadata used to load data by the `Client()` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Composition\n",
    "\n",
    "As above, each row of data represents one *unit* of training. To create a training batch from these individual examples, a number of variables are defined in the `batch` attribute of the `Client()` `YAML` file, which together specify the composition of each data batch used for algorithm training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "batch:\n",
    "  fold: -1\n",
    "  sampling:\n",
    "    fg: 0.5\n",
    "    bg: 0.5 \n",
    "```\n",
    "\n",
    "The composition (source) of each data batch used for network training derives from two primary factors:\n",
    "\n",
    "* cross validation fold for current experiment\n",
    "* stratified sampling (if any)\n",
    "\n",
    "### Validation fold\n",
    "\n",
    "For a given experiment, the specified `fold` represents the data split to use for validation. For example if fold is set to 0, then all rows with a value of 0 in the `valid` column will be used for validation. Set the fold to -1 in order to use **all data** for both training and validation (e.g. no validation). By default, the dataset is split into five validation folds. To perform `N`-fold cross validation, repeat each experiment a total of `N` times where the fold is set to an integer between `0` and `N-1`. \n",
    "\n",
    "### Stratified sampling\n",
    "\n",
    "For certain experiments, it is critical for specific rows to be used during training at a higher or lower frequency than others. This technique is known as stratified sampling. A common example is to oversample from the images containing a positive disease finding (e.g. pathology tends to be rare) so that the distribution of positive and negative training examples is balanced.\n",
    "\n",
    "The syntax to define stratified sampling is a series of key-value pairs such that:\n",
    "\n",
    "* key: name of header column, containing a boolean vector, that defines rows which are part of a specific cohort\n",
    "* value: rate between `[0, 1]` for which to sample from this specific cohort\n",
    "\n",
    "All values in total must add to 1.0.\n",
    "\n",
    "In the example above, the `fg` column contains a value of True or False for each row depending on whether or not a positive mask value is present in the corresponding label. The `bg` column contains the opposite information. \n",
    "\n",
    "### Batch size\n",
    "\n",
    "Use this field to specify the number of training examples (batch size) used for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Specifications\n",
    "\n",
    "```yml\n",
    "specs:\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms:\n",
    "        clip: \n",
    "          min: 0\n",
    "          max: 256\n",
    "        shift: 64\n",
    "        scale: 64\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ...\n",
    "```\n",
    "\n",
    "The `specs` entry in the configuration file defines the type, shape and origin of data to be loaded, as well as any necessary normalization operations to perform on the loaded data. The two primary entries in `specs` are `xs` and `ys` which represent training input data and output label(s), respectively. For each entry `xs` and `ys`, one or many individual volumes may be defined depending on architecture requirements.\n",
    "\n",
    "For each individual volume, the following parameters must be defined:\n",
    "\n",
    "* `dtype`: str representing the data type (often `float32` for input data, `uint8` for output label)\n",
    "* `shape`: 4D shape of input (Z x Y x X x channel); note channel often == 1; for 2D data, Z == 1\n",
    "* `loads`: column (if any) to load data from (see below for more details)\n",
    "* `norms`: normalization parameter (see below for more details); use `null` keyword to indicate no normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "The `loads` entry determines the column name from which to populate data for this input or output variable. If the column is part of the `fnames` DataFrame then the corresponding file will be loaded (at the slice location specified by `coord` if the data is 3D or 4D). If the column is part of the `header` DataFrame then the raw value will be coverted to a Numpy array.\n",
    "\n",
    "If no corresponding data should be loaded, use the keyword `null`. In this scenario, the corresponding array should be dynamically defined in an overloaded `client.preprocess(...)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization parameters\n",
    "\n",
    "A total of up to three parameters can be used to define a data normalization strategy:\n",
    "\n",
    "* clip (includes min and/or max)\n",
    "* shift\n",
    "* scale\n",
    "\n",
    "These parameters are implemented in the following normalization formula:\n",
    "\n",
    "```\n",
    "arr = (arr.clip(min=..., max=...) - shift) / scale\n",
    "```\n",
    "\n",
    "There are three ways to define these normalization parameters:\n",
    "\n",
    "1. **Constant value (integer or float)**. Use a literal value if you know that the required parameter value is constant for all data. This is most commonly used for CT imaging data (voxel values are precalibrated as Hounsfeld Units).\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  clip: \n",
    "    min: 0\n",
    "    max: 256\n",
    "  shift: 64\n",
    "  scale: 64\n",
    "```\n",
    "\n",
    "2. **Numpy function (@ keyword)**. Use a keyword prefixed by `@` to represent valid Numpy function (e.g. `np.(...)`) to be applied dynamically to each input image upon load. Common usage here includes the `@mean` and `@std` functions to implement a simple z-score transformation. *Important*: these methods should be used only if the input image is guaranteed to provide valid return(s); as a counterexample, the standard deviation on a uniform 2D image (seen at the top and bottom of 3D volumes) will be undefined. Thus this should rarely be used for 3D or 4D volumes (see option 3 below instead).\n",
    "\n",
    "```yml\n",
    "norms: \n",
    "  shift: @mean\n",
    "  scale: @std\n",
    "```\n",
    "\n",
    "3. **Column name**. Use a regular str (no @ prefix) to indicate a column name containing any custom normalization parameters. For 3D volumes, a common strategy is to normalize each slice by the mean and standard deviation of the entire volume. However, since loading the entire 3D volume for each slice during training is inefficient, volume statistics can instead be stored in each row of the `*.csv` file and simply referenced for preprocessing. A z-score normalization implemented using this technique is the recommended approach for MR imaging data.\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  shift: mu\n",
    "  scale: sd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "As an alternative to the above, occasionally the data needs to be altered via a mapping of current to target values. This is most commonly performed to transform model outputs. For example, if the serialized label contains a total of 5 values (0, 1, 2, 3, 4) and you wish to binarize the output into class 0 (0, 2, 4) and class 1 (1, 3), the following mapping can be specified:\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  mapping:\n",
    "    0: 0\n",
    "    1: 1\n",
    "    2: 0\n",
    "    3: 1\n",
    "    4: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overloading `Client()`\n",
    "\n",
    "While the majority of common data loading permutations can be accomplished by simple modifications to the client `YAML` file, occasionally additional Python code will be needed to enhance functionality. This is especially true if you wish to perform custom data augmentation, or you need to manually create a custom mask array for modifying / weighting the loss function.\n",
    "\n",
    "To accomplish these tasks, the easiest strategy is to overload the `preprocess(...)` method of the `Client()` class. The method allows a user to insert any arbitrary number of modifications to the loaded dataset prior to release (yielding) by the Python generator.\n",
    "\n",
    "To do so, either create a new `Client` class via object inheritance, or use the Jarvis `@overload` decorator:\n",
    "\n",
    "```python\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import overload\n",
    "\n",
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    ...\n",
    "    return arrays\n",
    "```\n",
    "\n",
    "Note that the signature for the `preprocess(...)` method includes a single Python dictionary, `arrays`, which consists of a nested dictionary containing both `xs` and `ys` variables, as well as additional metadata in the `kwargs` argument as needed. Within this method, **any** number of modifications to the `arrays` variable may be performed. Most commonly this will include code to modify the input(s) and/or output(s) pixel values, or to generate a custom training mask for modifying the loss function. Some additional special use cases are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new array\n",
    "\n",
    "To create a new array such a `msk` variable that is not already defined in the current `Client()` configuration, consider the following workflow:\n",
    "\n",
    "1. Update the client `YAML` file to generate the new array under `specs`. As above, the `loads: null` attribute can be specified such that an empty array is generated (does not *load* any existing column of data).\n",
    "\n",
    "```yml\n",
    "specs:\n",
    "  xs:\n",
    "    dat: ... (existing from above) ...\n",
    "    msk:\n",
    "      dtype: float32\n",
    "      loads: null\n",
    "      norms: null\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ...\n",
    "```\n",
    "\n",
    "2. Overload the `Client`, ensuring to populate `arrays['msk']` with the appropriate data\n",
    "\n",
    "```python\n",
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \n",
    "    # --- Create arrays['msk']\n",
    "    arrays['msk'] = arrays['lbl'] > 0\n",
    "    \n",
    "    return arrays\n",
    "```\n",
    "\n",
    "Note that this approach is preferred, as the correct Tensorflow / Keras `Input(...)` will be automatically created when invoking the `client.get_inputs(...)` method. \n",
    "\n",
    "If the `yml` if is **not** updated, then `client.get_inputs(...)` will only yield the inputs that are currently defined. Thus, the new `msk` variable will need to be manually created:\n",
    "\n",
    "```python\n",
    "# --- Create existing inputs\n",
    "from tensorflow.keras import Input\n",
    "inputs = client.get_inputs(Input)\n",
    "\n",
    "# --- Add additional dictionary entry for new msk\n",
    "inputs['msk'] = Input(shape=..., name='msk')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying array shape\n",
    "\n",
    "Occasionally the input shape loaded by the `Client()` is not the final shape you wish to use in the model training. For example, you may want to load a multi-slice input, peform some sort of data augmentation (including oblique projection out of the currently z-axis plane), and then extract out just the middle slice for training. To accomplish this, consider the following workflow:\n",
    "\n",
    "1. Update the client `YAML` file to indicate that the final `input` shape is different than the shape of the `saved` data:\n",
    "\n",
    "```yml\n",
    "specs:\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms: ... (from above) ...\n",
    "      shape:\n",
    "        input: [1, 512, 512, 1]\n",
    "        saved: [3, 512, 512, 1]\n",
    "```\n",
    "\n",
    "2. Overload the `Client`, ensuring to populate `arrays['dat']` with the appropriate data shape\n",
    "\n",
    "```python\n",
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \n",
    "    # --- Perform some sort of modification to arrays['dat']\n",
    "    pass\n",
    "\n",
    "    # --- Finalize arrays['dat'] from 3-slices to 1-slice\n",
    "    arrays['dat'] = arrays['dat'][1:-1]\n",
    "    \n",
    "    return arrays\n",
    "```\n",
    "\n",
    "Note that it is critical to overload the `Client` when specifying different `input` and `saved` shapes, otherwise the data yielded by the Python generator will not match the `Input` shape expected by the Tensorflow / Keras model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
