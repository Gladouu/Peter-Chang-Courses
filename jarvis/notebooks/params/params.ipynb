{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, we will explore several strategies for optimizing Jupyter notebook code and hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "from jarvis.train import datasets, params\n",
    "from jarvis.train.client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of (frontal projection) chest radiographs from a subset of the RSNA / Kaggle pneumonia challenge (https://www.kaggle.com/c/rsna-pneumonia-detection-challenge). From the complete cohort, a random subset of 1,000 exams will be used for training and evaluation.\n",
    "\n",
    "### Download\n",
    "\n",
    "The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/xr_pna`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "paths = datasets.download(name='xr/pna-512')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the location of downloaded data is not required for baseline functionality, manipulation of the underlying data structures and configuration files may be needed for implementing customized training. As a result it may be useful to note this location for future reference. All code and key configuration files are located relative to a project `code` root directory. This directory can be retrieved via the `paths` variable that is returned from the `datasets.download(...)` call, which is comprised of a Python dictionary with two entries:\n",
    "\n",
    "* `paths['code']`: root directory of all Python code and configuration files\n",
    "* `paths['data']`: root directory of all raw data\n",
    "\n",
    "Note that by default, these two directories are identical and placed in `/data/raw/[datatset_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "While interactive coding (encouraged by the Jupyter notebook design) is well-suited for initial algorithm debugging and testing, training a model accross a grid-search of hyperparameters can be very challenging without a strategy that helps automate and keep track of experiments. In this tutorial, we will explore one potential mechanisms using prebuilt `jarvis.train.params` module.\n",
    "\n",
    "To begin, let us create a `*.csv` file where each *column* represents a potential hyperparameter to test and each *row* represents one possible experimental configuration. Let us look at one possible such file now:\n",
    "\n",
    "```\n",
    "output_dir | fold | batch_size | LR       | alpha | iterations\n",
    "-----------------------------------------------------------------\n",
    "./exp01-0  | 0    | 8          | 0.0002   | 1     | 20000\n",
    "./exp01-1  | 1    | 8          | 0.0002   | 1     | 20000\n",
    "./exp01-2  | 2    | 8          | 0.0002   | 1     | 20000\n",
    "./exp01-3  | 4    | 8          | 0.0002   | 1     | 20000\n",
    "./exp01-4  | 5    | 8          | 0.0002   | 1     | 20000\n",
    "./exp02-0  | 5    | 8          | 0.0002   | 2     | 20000\n",
    "```\n",
    "\n",
    "In this example, we will consider the following hyperparameters:\n",
    "\n",
    "* **output_dir**: location to save outputs\n",
    "* **fold**: cross-validation fold\n",
    "* **batch_size**: training batch size\n",
    "* **LR**: learning rate\n",
    "* **alpha**: multiplication factor for number of channels\n",
    "* **iterations**: number of training iterations\n",
    "\n",
    "**Important**: we will create the following `*.csv` file here programatically for demonstration purposes, *however* in practice you should create the file independently outside of Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_csv(fname='./hyper.csv', overwrite=False):\n",
    "    \n",
    "    if os.path.exists(fname) and not overwrite:\n",
    "        return\n",
    "    \n",
    "    df = {'output_dir': [], 'fold': [], 'batch_size': [], 'LR': [], 'alpha': [], 'iterations': []}\n",
    "    \n",
    "    # --- Create exp01\n",
    "    for fold in range(5):\n",
    "        df['output_dir'].append('./exp01-{}'.format(fold))\n",
    "        df['fold'].append(fold)\n",
    "        df['batch_size'].append(8)\n",
    "        df['LR'].append(0.0002)\n",
    "        df['alpha'].append(1)\n",
    "        df['iterations'] = 20000\n",
    "        \n",
    "    # --- Create exp02\n",
    "    fold = 0\n",
    "    df['output_dir'].append('./exp02-{}'.format(fold))\n",
    "    df['fold'].append(fold)\n",
    "    df['batch_size'].append(8)\n",
    "    df['LR'].append(0.0002)\n",
    "    df['alpha'].append(2)\n",
    "    df['iterations'] = 20000  \n",
    "    \n",
    "    # --- Save *.csv file\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(fname, index=False)\n",
    "    \n",
    "    print('Created {} successfully'.format(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create \n",
    "create_hyper_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this `*.csv` file, simple use the `params.load(...)` function in `jarvis.train.params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load params\n",
    "p = params.load(csv='./hyper.csv', row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the parameters for **row 0** are availabe in the dictionary `p`. \n",
    "\n",
    "Additionally, it should be noted that if certain OS environmental variables are set prior to running this notebook, the `params.load(...)` function will *ignore* the kwargs passed in this code, and instead use the `csv` and `row` specifications designed by the environment variables instead:\n",
    "\n",
    "```\n",
    "$ export JARVIS_PARAMS_CSV=hyper.csv\n",
    "$ export JARVIS_PARAMS_ROW=0 \n",
    "$ python [training_script].py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client\n",
    "\n",
    "The `Client()` object helps facilitate a simple interface for loading data efficiently for model training. By default, one (or several) `Client()` `*.yml` files are provided as a template with baseline configurations (e.g. data matrix shape, normalization, preprocessing, etc). By passing a template `*.yml` file into the `Client()` constructor, one can create a default object (and Python generator) to use for training:\n",
    "\n",
    "```python\n",
    "CLIENT_TEMPLATE = '{}/data/ymls/client.yml'.format(paths['code'])\n",
    "client = Client(CLIENT_TEMPLATE)\n",
    "```\n",
    "\n",
    "During the course of training, the `Client()` object will aggregate additional specific information regarding the current state of data (e.g. number of passes through each data sample, epoch-specific randomization, etc). At any given time during training, the client state may be saved using:\n",
    "\n",
    "```python\n",
    "# --- Save client state\n",
    "CLIENT_TRAINING = '/path/to/save/client.yml'\n",
    "client.to_yml(CLIENT_TRAINING)\n",
    "```\n",
    "\n",
    "Upon resuming a training session, this new *experiment specific* `Client()` `*.yml` file can be directly loaded instead of the default template `*.yml` file to ensure that the training resumes with the exact same data sample and randomization as before. To account for this flexibility, use the following modification to the `Client()` initialization code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize client\n",
    "CLIENT_TEMPLATE = '{}/data/ymls/client-cls-512.yml'.format(paths['code'])\n",
    "CLIENT_TRAINING = '{}/client.yml'.format(p['output_dir'])\n",
    "\n",
    "client = Client(CLIENT_TRAINING if os.path.exists(CLIENT_TRAINING) else CLIENT_TEMPLATE, configs={\n",
    "    'batch': {\n",
    "        'size': p['batch_size'],\n",
    "        'fold': p['fold']}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice additionally that several hyperparameters from the `p` dictionary are referenced during `Client()` initialization here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create gen_train, gen_valid\n",
    "gen_train, gen_valid = client.create_generators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model inputs\n",
    "inputs = client.get_inputs(Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Let us first define a standard classifier network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=2)))\n",
    "\n",
    "# --- Define concatenation\n",
    "concat = lambda a, b : layers.Concatenate()([a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 2D U-net backbone, we will add a reference the `p['alpha']` hyperparameter that will modify the relative size of filter channel depth at all points through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract alpha value\n",
    "a = p['alpha']\n",
    "\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(int(8  * a), inputs['dat'])\n",
    "l2 = conv1(int(16 * a), conv2(int(16 * a), l1))\n",
    "l3 = conv1(int(24 * a), conv2(int(24 * a), l2))\n",
    "l4 = conv1(int(32 * a), conv2(int(32 * a), l3))\n",
    "l5 = conv1(int(40 * a), conv2(int(40 * a), l4))\n",
    "l6 = conv1(int(48 * a), conv2(int(48 * a), l5))\n",
    "l7 = conv1(int(56 * a), conv2(int(56 * a), l6))\n",
    "l8 = conv1(int(64 * a), conv2(int(64 * a), l7))\n",
    "\n",
    "# --- Flatten\n",
    "f0 = layers.Flatten()(l8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create logits\n",
    "logits = {}\n",
    "logits['pna'] = layers.Dense(2, name='pna')(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Putting everything together, use the following cell to create and compile the convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "# --- Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=p['LR']), \n",
    "    loss={'pna': losses.SparseCategoricalCrossentropy(from_logits=True)}, \n",
    "    metrics={'pna': 'sparse_categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory for faster training\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Before we start training, let us load an existing model files in the `output_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create output_dir\n",
    "os.makedirs(p['output_dir'], exist_ok=True)\n",
    "\n",
    "# --- Load existing model if present\n",
    "MODEL_NAME = '{}/model.hdf5'.format(p['output_dir'])\n",
    "if os.path.exists(MODEL_NAME):\n",
    "    print('Loading existing model weights: {}'.format(MODEL_NAME))\n",
    "    model.load_weights(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up a training loop, consider the following configurations (certainly these may be modified programmatically via `params` as above if desired):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assume a 1000:250 ratio of train:valid\n",
    "steps_per_epoch = 250\n",
    "validation_freq = 4\n",
    "\n",
    "# --- Determine total loop iterations needed\n",
    "N = int(p['iterations'] / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this information, we will set up a training loop that saves the `client.yml` and `model.hdf5` intermediates after each loop iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(N):\n",
    "    \n",
    "    # --- Train\n",
    "    model.fit(\n",
    "        x=gen_train, \n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_data=gen_valid,\n",
    "        validation_steps=steps_per_epoch,\n",
    "        validation_freq=validation_freq)\n",
    "    \n",
    "    # --- Save model\n",
    "    model.save(MODEL_NAME)\n",
    "    \n",
    "    # --- Save client\n",
    "    client.to_yml(CLIENT_TRAINING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
