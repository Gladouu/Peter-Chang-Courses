{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Congratulations! You have reached the end of the CS 190 curriculum. As a final summative exercise, you will be tasked to develop a model to detect pulmonary infection (pneumonia) on chest radiographs using any of the approaches and tools you have learned this quarter. The brief tutorial will simply introduce the dataset and configuration of the Python generators prepared for this exercise. In addition, I will provide some hints and suggestions for how to build the best possible network using this data. Once you are familiar with the task, you are welcome to move onto the assignment which contains more details regarding algorithm design requirements and submission.\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive\n",
    "\n",
    "The Google Colab environment is transient and will reset after any prolonged break in activity. To retain important and/or large files between sessions, use the following lines of code to mount your personal Google drive to this Colab instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # --- Mount gdrive to /content/drive/My Drive/\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: it is not required to mount your Google Drive locally to run this tutorial. This is only required if you wish to save any trained model files or outputs. If you choose to skip this step, all remaining code below will compile without issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial we will use the following global `MOUNT_ROOT` variable to reference a location to store long-term data. If you are using a local Jupyter server and/or wish to store your data elsewhere, please update this variable now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set data directory\n",
    "MOUNT_ROOT = '/content/drive/My Drive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Tensorflow library version\n",
    "\n",
    "This tutorial will use the Tensorflow 2.1 library. Use the following line of code to select and download this specific version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select Tensorflow 2.x (only in Google Colab)\n",
    "% tensorflow_version 2.x\n",
    "% pip install tensorflow-gpu==2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from jarvis.train import datasets\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of (frontal projection) chest radiographs from the RSNA / Kaggle pneumonia challenge (https://www.kaggle.com/c/rsna-pneumonia-detection-challenge). The chest radiograph is the standard screening exam of choice to identify and trend changes in lung disease including infection (pneumonia). \n",
    "\n",
    "### Download\n",
    "\n",
    "The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/xr_pna`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='xr/pna-crp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python generators\n",
    "\n",
    "Once the dataset is downloaded locally, Python generators to iterate through the dataset can be easily prepared using the `datasets.prepare(...)` method. A `keyword` is needed if there are more than one dataset configurations prepared for training. In this tutorial, we will be using a cropped datatset which may be specified by passing the keyword string `crp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, gen_valid, client = datasets.prepare(name='xr/pna-crp', keyword='crp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created generators yield two variables for each iteration, `xs` and `ys`, each representing a dictionary of model input(s) and output(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "\n",
    "To help facilitate algorithm design, each original chest radiograph has been cropped to the right and left lungs individually, and resampled to a resolution of `(256, 128)`. There are a total of 53,298 individual images in this dataset.\n",
    "\n",
    "The `xs` dictionary contains the following entries:\n",
    "\n",
    "1. `dat`: input image at `(256, 128`) resolution\n",
    "2. `msk`: input mask to implement class weights / masked loss functions (optional)\n",
    "* 0 = background\n",
    "* 1 = lung\n",
    "\n",
    "The `ys` dictionary contains the following entries:\n",
    "\n",
    "1. `pna-cls`: a binary classification output\n",
    "* 0 = negative exam\n",
    "* 1 = positive exam (pneumonia)\n",
    "\n",
    "2. `pna-seg`: a binary segmentation output\n",
    "* 0 = background \n",
    "* 1 = pneumonia\n",
    "\n",
    "**Important**: while multiple input(s) and output(s) are available in this dataset, your are *not* required to use all of them. Simply incorporate whichever model input(s) and output(s) you find provide the overall best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('xs shape: {}'.format(xs['msk'].shape))\n",
    "print('ys shape: {}'.format(ys['pna-cls'].shape))\n",
    "print('ys shape: {}'.format(ys['pna-seg'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize both the image data and corresponding mask label using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example, msk\n",
    "xs, ys = next(gen_train)\n",
    "imshow(xs['dat'][0], xs['msk'][0], radius=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example, pna\n",
    "xs, ys = next(gen_train)\n",
    "imshow(xs['dat'][0], ys['pna-seg'][0], radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `imshow(...)` function to create an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images, msk\n",
    "imshow(xs['dat'], xs['msk'], figsize=(12, 12), radius=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images, pna\n",
    "imshow(xs['dat'], ys['pna-seg'], figsize=(12, 12), radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inputs\n",
    "\n",
    "For every input in `xs`, a corresponding `Input(...)` variable can be created and returned in a `inputs` dictionary for ease of model development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model inputs\n",
    "inputs = client.get_inputs(Input)\n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs['dat'].shape)\n",
    "print(inputs['msk'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The goal of this project is to perform **global classification** for each image. In other words, regardless of algorithm choice, the final objective is to determine the absence or presence of pneumonia for each sample. This does **not** mean that you are required to use classification networks only; in fact it very well may be the case that a localization algorithm will overall perform better on this task.\n",
    "\n",
    "The task is designed to be open-ended on purpose. The only requirements are to:\n",
    "\n",
    "* test at minimum three different network architectures\n",
    "* one algorithm must use (at least) a classification type loss function\n",
    "* one algorithm must use (at least) a segmentation type loss function\n",
    "\n",
    "Note that the qualifier *at least* indicates that if you choose, you can use both classification and segmentation losses simultaneously for one algorithm to satisfy both requirements (this would allow you to test only non-classification / non-segmentation architectures for all remaining models if you choose).\n",
    "\n",
    "While you can choose to be creative and employ any architecture that you like, the following discussion may help guide your development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "For your classifier, any architecture covered in class (or any novel architecture you wish to implement) can be used. Due to the relatively large dataset size, creative network topologies may be used.\n",
    "\n",
    "**Important** Given that pneumonia infection can often be a very subtle finding, my recommendation is to use **more** convolutional layers (and filter channels) in the initial portion of your algorithm. These features in the early layers are going to represent fine details / high frequency patterns that are critical to good model performance. \n",
    "\n",
    "Assuming you are using the `lambda` notation described throughout class, then for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define model with extra initial layers\n",
    "l1 = conv1(32, conv1(32, inputs['dat']))\n",
    "l2 = conv1(48, conv1(48, conv2(48, l1)))\n",
    "l3 = conv1(64, conv1(64, conv2(64, l2)))\n",
    "\n",
    "# ... and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with various permutations. This recommendation to use extra parameters in the early layers will generally hold true for all model architectures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "\n",
    "Contracting-expanding architectures have many advantages over standard classification networks. Furthermore, to some extent, class imbalance has been mitigated by cropping the images to the right and left lungs individually. Thus, although lung masks have been provided (and likely may provide a slight incremental benefit over no masks), they are not required for this task.\n",
    "\n",
    "Keep in mind that as stated above, we are not interested in the segmentation mask itself. Instead we want to eventually perform binary classification for all networks. To collapse a final segmentation mask into a binary prediction, consider that for the most part, negative exams will contain relatively smaller (or empty) masks while positive exams will contain larger masks. Thus the goal is to find a threshold for which differentiation may be optimized. Importantly keep in mind that this threshold should be determined based on the **training set** (not validation set), and then applied after to the test set.\n",
    "\n",
    "To perform this task, first run inference through the train set to approximate the distribution of mask sizes between the two groups (pneumonia, no pneumonia). Given the large train set, it is not necessary to run through all cases (though for absolute highest precision this can be done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run model prediction for segmentation masks\n",
    "test_train, test_valid = client.create_generators(test=True)\n",
    "\n",
    "nnzs = []\n",
    "true = []\n",
    "\n",
    "for xs, ys in test_train:\n",
    "    \n",
    "    logs = model.predict(xs)\n",
    "    pred = np.argmax(logs[1], axis=-1)\n",
    "    \n",
    "    # --- Remove masked pixels if needed\n",
    "    pred[xs['msk'][..., 0] == 0] = 0\n",
    "    \n",
    "    # --- Record number of pixels in mask\n",
    "    nnzs.append(np.count_nonzero(pred))\n",
    "    true.append(ys['pna-cls'].squeeze())\n",
    "    \n",
    "    # --- Break after 1000 exams\n",
    "    if len(nnzs) == 1000:\n",
    "        break\n",
    "\n",
    "# --- Convert to arrays\n",
    "nnzs = np.array(nnzs)\n",
    "true = np.array(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to determine a threshold, start by comparing means as a good initial check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print mean of the two groups\n",
    "print(nnzs[true == 0].mean())\n",
    "print(nnzs[true == 1].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, choose a threshold that seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred, true, treshold):\n",
    "    \"\"\"\n",
    "    Method to calculate accuracy at given threshold of mask pixels\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Calculate true positives / true negatives\n",
    "    tp = np.count_nonzero(nnzs[true == 1] >= threshold)\n",
    "    tn = np.count_nonzero(nnzs[true == 0] < threshold)\n",
    "\n",
    "    # --- Accuracy\n",
    "    accuracy = (tp + tn) / nnzs.size\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print(calculate_accuracy(pred, true, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As course, as desired it may be reasonable to try a number of different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box localization\n",
    "\n",
    "For certain tasks, box localization networks may have advantages over segmentation architectures, primarily when dealing with class imbalance. Additionally the use of **focal** loss (which may be implemented in any architecture above) has an advantage to focus the algorithm on rare cases to further improve incremental model performance.\n",
    "\n",
    "Should you decide to use a box localization algorithm, consider that the pneumonia lesions in general tend to be quite large (the images are already cropped to the right and left lungs). Thus ensure that the parameters for box configuration are adjusted appropriately. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
