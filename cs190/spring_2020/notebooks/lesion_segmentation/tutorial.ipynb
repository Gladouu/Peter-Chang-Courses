{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will explore several strategies to address class imbalance as well as how to tune a network with weighted loss functions (e.g. class weights and masks). Strategies discussed include:\n",
    "\n",
    "* stratified sampling\n",
    "* pixel-level class weights\n",
    "* pixel-level masked loss\n",
    "\n",
    "Ultimately, the goal of this tutorial (and class assignment) is to create a high sensitivity detector for pulmonary infection (pneumonia) on chest radiographs. \n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive\n",
    "\n",
    "The Google Colab environment is transient and will reset after any prolonged break in activity. To retain important and/or large files between sessions, use the following lines of code to mount your personal Google drive to this Colab instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # --- Mount gdrive to /content/drive/My Drive/\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial we will use the following global `MOUNT_ROOT` variable to reference a location to store long-term data. If you are using a local Jupyter server and/or wish to store your data elsewhere, please update this variable now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set data directory\n",
    "MOUNT_ROOT = '/content/drive/My Drive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Tensorflow library version\n",
    "\n",
    "This tutorial will use the Tensorflow 2.1 library. Use the following line of code to select and download this specific version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select Tensorflow 2.x (only in Google Colab)\n",
    "% tensorflow_version 2.x\n",
    "% pip install tensorflow-gpu==2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, models, layers, metrics\n",
    "from jarvis.train import datasets, custom\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import overload, tools as jtools\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of (frontal projection) chest radiographs from the RSNA / Kaggle pneumonia challenge (https://www.kaggle.com/c/rsna-pneumonia-detection-challenge). The chest radiograph is the standard screening exam of choice to identify and trend changes in lung disease including infection (pneumonia). \n",
    "\n",
    "The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/xr_pna`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='xr/pna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality. As needed, pass any custom configurations (e.g. batch size, normalization parameters, etc) into the optional `configs` dictionary argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "configs = {'batch': {'size': 8}}\n",
    "gen_train, gen_valid, client = datasets.prepare(name='xr/pna', configs=configs, keyword='seg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created generators yield a total of `n` training samples based on the specified batch size. As before, each iteration yields two variables, `xs` and `ys`, each representing a dictionary of model input(s) and output(s). \n",
    "\n",
    "Compared to prior tutorials with just a single input and output, there are two separate inputs in the `xs` dictionary. Specifically a new array named `msk` is available as an additional `xs` input. The purpose of this `msk` object will be described in detail in the following tutorial. For now, note that the `msk` conforms to the boundaries of the right and left lungs individually.\n",
    "\n",
    "Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('xs shape: {}'.format(xs['msk'].shape))\n",
    "print('ys shape: {}'.format(ys['pna'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following lines of code to visualize both the image data and corresponding mask label using the `imshow(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example, msk\n",
    "xs, ys = next(gen_train)\n",
    "imshow(xs['dat'][0], xs['msk'][0], radius=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show the first example, pna\n",
    "xs, ys = next(gen_train)\n",
    "imshow(xs['dat'][0], ys['pna'][0], radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `montage(...)` function to create an N x N mosaic of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images, msk\n",
    "imshow(xs['dat'], xs['msk'], figsize=(12, 12), radius=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show \"montage\" of all images, pna\n",
    "imshow(xs['dat'], ys['pna'], figsize=(12, 12), radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D operations\n",
    "\n",
    "Note that the model input shapes for this exercise will be provided as 3D tensors. Even if your current model does not require 3D data (as in this current tutorial), all 2D tensors can be represented by a 3D tensor with a z-axis shape of 1. In addition, designing all models with this configuration (e.g. 3D operations) ensures that minimal code changes are needed when testing various 2D and 3D network architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Loss\n",
    "\n",
    "To implement custom loss weights (and/or masks), a generic `msk` array will be used to perform a point-wise multiplication against the final pixel-by-pixel loss. For locations where the loss should be **weighted**, use a constant value > 1, For locations where the loss should be ignored (**masked**), use a constant value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Client` object\n",
    "\n",
    "The `Client` object is used as an interface to training data. Among other useful features, it is used to create Python generators for model training:\n",
    "\n",
    "```python\n",
    "# --- Create training generators\n",
    "gen_train, gen_valid = client.create_generators()\n",
    "\n",
    "# --- Create testing generators\n",
    "test_train, test_valid = client.create_generators(test=True)\n",
    "```\n",
    "\n",
    "Note that in prior assignments, the use of the standard `datasets.prepare(...)` method will implicitly create a new `Client()` object and invoke the `create_generators(...)` function as shown above. While this approach is sufficient for many projects, if data yielded by the generator needs to be manually altered in any way, the original `Client()` object needs to be overloaded (modified). To facilitate modification, use the `@overload` Python decorator (as part of the `jarvis` library). See below for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom loss weights and masks\n",
    "\n",
    "As above, the `msk` array that is yielded by the generator will need to be customized to account for any desired weights and/or masked values. Recall also that in the above data exploratory step, the default `msk` array will be populated with a mask of the right (value == 1) and left (value == 2) lungs. \n",
    "\n",
    "To modify this baseline `msk` array, we will overload the `preprocess` method of the `Client` class. The `arrays` variable is a dictionary that conforms to the various `xs` and `ys` arrays used to train the model. In this particular experiment, the following array dictionary is defined (conforms to the exact same nomenclature as above):\n",
    "\n",
    "```python\n",
    "arrays = {\n",
    "    \n",
    "    # --- All xs input(s)\n",
    "    'xs': {\n",
    "        'dat': ...,\n",
    "        'msk': ...},\n",
    "    \n",
    "    # --- All ys output(s)\n",
    "    'ys': {\n",
    "        'pna': ...}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant 1**: Mask the loss function to ignore non-lung and non-pneumonia pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create a custom msk array for class weights and/or masks\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Create msk\n",
    "    msk = np.zeros(arrays['xs']['dat'].shape)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant 2**: Use class weights to increase penalty for pnuemonia pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create a custom msk array for class weights and/or masks\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Create msk\n",
    "    msk = np.ones(arrays['xs']['dat'].shape)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant 3**: Use a combination of both class weights and masked losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create a custom msk array for class weights and/or masks\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Create msk\n",
    "    msk = np.zeros(arrays['xs']['dat'].shape)\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "To manually create your new custom `Client` object, pass the location of the `client*.yml` file that defines all the configurations for your data client. This file is located in the `ymls/` directory of the data download. For convenience, the `jarvis.utils.general.tools` module (imported as `jtools` above) can be used to find this location automatically: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find client yml file\n",
    "yml = '{}/data/ymls/client-seg.yml'.format(jtools.get_paths('xr/pna')['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following block to create a new `Client` object and visualize the custom `msk`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Manually create Client\n",
    "client = Client(yml)\n",
    "\n",
    "# --- Manually create generators\n",
    "gen_train, gen_valid = client.create_generators()\n",
    "\n",
    "# --- Show\n",
    "xs, ys = next(gen_train)\n",
    "imshow(xs['dat'][0], xs['msk'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sampling\n",
    "\n",
    "The data consists of a total of three separate cohorts: negative; positive (infection); not normal but not infection. Note that the positive cases of infection account for approximately 1/5th (20%) of the data. As a result it will be necessary to increase the frequency of this cohort to optimize training efficiency.\n",
    "\n",
    "Furthermore, the third indeterminate category (e.g. not normal but not infection) is quite subjective. In general it is quite to difficult to differentiate between various disease processes on chest radiograph alone. As a result, in this particular algorithm (where the primary goal is to optimize for algorithm sensitivity), we will **ignore** this third category. \n",
    "\n",
    "To implement stratified sampling, pass the appropriate `sampling` specifications to the `configs` variable when creating the `Client()` object (this is the exact same configs variable used by the `datasets.prepare(...)` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configs dict\n",
    "configs = {\n",
    "    'batch': {'size': 8},\n",
    "    'sampling': {\n",
    "        'cohort-neg': 0.5,\n",
    "        'cohort-pna': 0.5}}\n",
    "\n",
    "# --- Manually create Client\n",
    "client = Client(yml, configs=configs)\n",
    "\n",
    "# --- Manually create generators\n",
    "gen_train, gen_valid = client.create_generators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "To localize pneumonia (lung infection) on chest radiographs, we will implement a standard contracting-expanding network (e.g. U-Net). Note that the original model input shape of `(512, 512)` is larger than in prior exercises; as a result a total of 6 subsampling blocks will be used. In the assignment, feel free to try various architecture permutations.\n",
    "\n",
    "### Create Inputs\n",
    "\n",
    "As before, use the `client.get_inputs(...)` to create model inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create inputs\n",
    "inputs = client.get_inputs(Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.LeakyReLU()(x)\n",
    "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define contracting layers\n",
    "l1 = conv1(8, inputs['dat'])\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(32, conv2(32, l2))\n",
    "l4 = conv1(48, conv2(48, l3))\n",
    "l5 = conv1(64, conv2(64, l4))\n",
    "l6 = conv1(80, conv2(80, l5))\n",
    "\n",
    "# --- Define expanding layers\n",
    "l7  = tran2(64, l6)\n",
    "l8  = tran2(48, conv1(64, l7  + l5))\n",
    "l9  = tran2(32, conv1(48, l8  + l4))\n",
    "l10 = tran2(16, conv1(32, l9  + l3))\n",
    "l11 = tran2(8,  conv1(16, l10 + l2))\n",
    "l12 = conv1(8,  conv1(8,  l11 + l1))\n",
    "\n",
    "# --- Create logits\n",
    "logits = {}\n",
    "logits['pna'] = layers.Conv3D(filters=2, name='pna', **kwargs)(l12)\n",
    "\n",
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model\n",
    "\n",
    "To compile this model, several custom `loss` and `metrics` objects will need to be defined.\n",
    "\n",
    "#### Loss\n",
    "\n",
    "As in prior tutorials, a standard (sparse) softmax cross-entropy loss will be used to optimize the segmentation model. A custom softmax cross entropy loss function is available as part of the `jarvis.train.custom` module to implement the necessary modifications for weighted and/or masked loss functions. To use this object, simply pass the `inputs['msk']` array as the first argument into the loss function initializer.\n",
    "\n",
    "For many common loss functions, the low-level Tensorflow or Keras loss object does support weighted loss calculations, however are not availabe by default using the standard `model.fit(...)` API. To accomodate this, Python closures can be used to create a wrapper around the default loss function calculation:\n",
    "\n",
    "```python\n",
    "def sce(weights, scale=1.0):\n",
    "\n",
    "    loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def sce(y_true, y_pred):\n",
    "\n",
    "        return loss(y_true=y_true, y_pred=y_pred, sample_weight=weights) * scale\n",
    "\n",
    "    return sce \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create custom weighted loss\n",
    "loss = {'pna': custom.sce(inputs['msk'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "The goal of this model is to optimize for overall algorithm sensitivity. However, to ensure that the model simply does not predict positive for *every* pixel (this would in fact yield a sensitivity of 100%), we will ensure that the overall Dice score metric remains within a reasonable value.\n",
    "\n",
    "A series of custom metrics including Dice score and sensitivity calculation are availabe as part of the `jarvis.train.custom` module to implement weighted and/or masked metrics. To use this object, simply pass the `inputs['msk']` array as the first argument into the metrics initializer. Since we are using two separate metrics in this example, pass both as part of a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create metrics\n",
    "metrics = custom.dsc(weights=inputs['msk'])\n",
    "metrics += [custom.softmax_ce_sens(weights=inputs['msk'])]\n",
    "\n",
    "metrics = {'pna': metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory data\n",
    "\n",
    "For moderate sized datasets which are too large to fit into immediate hard-drive cache, but small enough to fit into RAM memory, it is often times a good idea to first load all training data into RAM memory for increased speed of training. The `client` can be used for this purpose as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory for faster training\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Important*: For the current dataset, which is relatively large, your Google Colab instance may not be able to load all data into memory. If so, just continue on to training below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "To use Tensorboard, create the necessary Keras callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks  \n",
    "tensorboard_callback = callbacks.TensorBoard('./logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train model\n",
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=50, \n",
    "    epochs=120,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=50,\n",
    "    validation_freq=4,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Tensorboard\n",
    "\n",
    "After running several iterations, start Tensorboard using the following cells. After Tensorboard has registered the first several checkpoints, subsequent data will be updated automatically (asynchronously) and model training can be resumed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext tensorboard\n",
    "% tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `model.predict(...)` to obtain logit scores\n",
    "* compare prediction with ground-truth (Dice score, sensitivity)\n",
    "* serialize in Pandas DataFrame\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, we will create new **test mode** data generators, which will simply load each example individually once for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run prediction on a single (first) example from the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run a single prediction\n",
    "x, y = next(test_valid)\n",
    "logits = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the predicted results. Recall that the `np.argmax(...)` function can be used to convert raw logit scores to predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create prediction\n",
    "pred = np.argmax(logits[0], axis=-1)\n",
    "\n",
    "# --- Show\n",
    "imshow(x['dat'][0], pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint** What is the problem with this mask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that during training, the algorithm is never penalized regardless of class for predictions *outside of the mask* (e.g. values == 0) used for training. Thus, to generate the final prediction, one needs to similarly remove the masked values of the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean up pred using mask\n",
    "pred[x['msk'][0, ..., 0] == 0] = 0\n",
    "\n",
    "# --- Show\n",
    "imshow(x['dat'][0], pred[0], radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better. Let us look at the ground-truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show\n",
    "imshow(x['dat'][0], y['pna'][0], radius=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for sensitivity\n",
    "\n",
    "In addition to evaluating overall model Dice score, the goal of this exercise is to create a high-sensitivity model. Recall that sensitivity is defined as the number of TP predictions / all positive exams (e.g. proportion of positive findings that are correctly identified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sens(pred, true):\n",
    "    \"\"\"\n",
    "    Method to calculate sensitivity from pred and true masks\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate sens\n",
    "calculate_sens(\n",
    "    pred=pred,\n",
    "    true=y['pna'][0, ..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True)\n",
    "\n",
    "for x, y in test_valid:\n",
    "    \n",
    "    # --- Create prediction\n",
    "    pred = np.argmax(logits[0], axis=-1)\n",
    "    \n",
    "    # --- Clean up pred using mask\n",
    "    pred[x['msk'][0, ..., 0] == 0] = 0\n",
    "    \n",
    "    # --- Calculate Dice\n",
    "    dice = ...\n",
    "    \n",
    "    # --- Calculate sens\n",
    "    sens = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define columns\n",
    "df = pd.DataFrame(...)\n",
    "df['dice'] = ...\n",
    "df['sens'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "model.save('./lesion_segmentation.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model('./lesion_segmentation.hdf5', compile=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
