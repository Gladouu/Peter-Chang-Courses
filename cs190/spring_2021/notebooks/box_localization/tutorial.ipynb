{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial we will explore the foundations of box localization networks. First we will examine the most common anchor parameterization of boxes at various scales and ratios across different feature map levels. Second we will explore the a popular backbone common to many modern box localization networks: the feature pyramid network. Finally we will dive into specifies regarding a popular high-performing implementation: RetinaNet.\n",
    "\n",
    "This tutorial is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found at: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56d3oMiMw8Wm"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "The following lines of code will configure your Google Colab environment for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU runtime\n",
    "\n",
    "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
    "\n",
    "```\n",
    "Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jarvis library\n",
    "\n",
    "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install jarvis (only in Google Colab or local runtime)\n",
    "% pip install jarvis-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Use the following lines to import any additional needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from tensorflow import losses, optimizers\n",
    "from tensorflow.keras import Input, Model, models, layers, metrics\n",
    "from jarvis.train import datasets, custom\n",
    "from jarvis.train.box import BoundingBox\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this tutorial will consist of brain tumor MRI exams derived from the MICCAI Brain Tumor Segmentation Challenge (BRaTS). More information about he BRaTS Challenge can be found here: http://braintumorsegmentation.org/. Each single 2D slice will consist of one of four different sequences (T2, FLAIR, T1 pre-contrast and T1 post-contrast). In this exercise, we will use this dataset to derive a model for slice-by-slice tumor segmentation. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/mr_brats_2020`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download dataset\n",
    "datasets.download(name='mr/brats-2020-mip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded, the `datasets.prepare(...)` method can be used to generate the required python Generators to iterate through the dataset, as well as a `client` object for any needed advanced functionality.\n",
    "\n",
    "To specificy the correct Generator template file, pass a designated `keyword` string. In this tutorial, we will be using brain MRI volumes that have been preprocessed using a *mean intensity projection* (MIP) algorithm to subsample the original 155-slice inputs to 40-50 slices, facilitating ease of algorithm training within the Google Colab platform. In addition we will be performing voxel-level tumor prediction (e.g., a prediction for every single voxel in the 3D volume). To select the correct Client template for this task, use the keyword string `mip*vox`. \n",
    "\n",
    "Finally, for sake of simplicity, this tutorial will binarize the ground-truth labels (instead of the original four separate tumor classes). To do so, pass the following `configs` dictionary into the `datasets.prepare(...)` method. As needed, modify the custom `configs` dictionary with additional configurations as needed (e.g. batch size, normalization parameters, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "configs = {'specs': {'ys': {'tumor': {'norms': {'clip': {'max': 1}}}}}}\n",
    "gen_train, gen_valid, client = datasets.prepare(name='mr/brats-2020-mip', keyword='mip*vox', configs=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, each iteration yields two variables, `xs` and `ys`, each representing a dictionary of model input(s) and output(s). In the current example, there is just a single input and output. Let us examine the generator data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yield one example\n",
    "xs, ys = next(gen_train)\n",
    "\n",
    "# --- Print dict keys\n",
    "print('xs keys: {}'.format(xs.keys()))\n",
    "print('ys keys: {}'.format(ys.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print data shape\n",
    "print('xs shape: {}'.format(xs['dat'].shape))\n",
    "print('ys shape: {}'.format(ys['tumor'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D operations\n",
    "\n",
    "Note that the model input shapes for this exercise (and all subsequent exercises) will be provided as 3D tensors. Even if your current model does not require 3D data (as in this current tutorial), all 2D tensors can be represented by a 3D tensor with a z-axis shape of 1. In addition, designing all models with this configuration (e.g. 3D operations) ensures that minimal code changes are needed when testing various 2D and 3D network architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Parameterization\n",
    "\n",
    "Recall the common parameterization of boxes across an image using a grid of anchors:\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/box_params.png)\n",
    "\n",
    "At each anchor location, a total of **A** anchors may be defined spanning a variety of:\n",
    "\n",
    "* **aspect ratios**: 1:1, 2:1, 1:2, etc ...\n",
    "* **scales**: 2 ** 0, 2 ** (1/3), 2 ** (2/3), etc ...\n",
    "\n",
    "For each **A** number of anchors, there are two separate predictions:\n",
    "\n",
    "* **K**-element logit score representing a binary prediction of whether or not the *k-th* class is present in the box\n",
    "* **4**-element box fine-tuning representing the shift in the height, width, y- and x-coordinates from base box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor grid sizes\n",
    "\n",
    "The H x W size of an anchor grid (e.g. and thus implicitly the correspond box size) is commonly referenced by the **number of subsamples** required relative to the original full image shape. For example, if an original image is (N, N) in shape, then the first subsampled feature map is (N / 2, N / 2), the second subsampled featured map is (N / 4, N / 4), etc...\n",
    "\n",
    "In this example, the original input images are (240, 240) MR images. Thus the following feature maps (prefixed with `c`) may be defined:\n",
    "\n",
    "* **c1**: 120 x 120 anchor grid\n",
    "* **c2**: 60 x 60 anchor grid\n",
    "* **c3**: 30 x 30 anchor grid\n",
    "* **c4**: 15 x 15 anchor grid\n",
    "\n",
    "... and so on. By default, it is most common to start at the `c2` or `c3` level and proceed to include 3 to 5 different resolutions depending on the desired target.\n",
    "\n",
    "### `BoundingBox`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BoundingBox` class as part of the `jarvis-md` library facilitates definition and manipulation of boxes parameterized using the above standard notation. The object initializer has the following arguments:\n",
    "\n",
    "```\n",
    "(iter)   image_shape     : original 2D image shape\n",
    "(int)    classes         : number of non-background classes\n",
    "(iter)   c               : feature maps to use; c1 = 1st subsample, c2 = 2nd subsample, etc\n",
    "(iter)   anchor_shapes   : base shape of anchors in each feature map\n",
    "(iter)   anchor_scales   : scales of each anchor parameterized as 2 ** (i/3)\n",
    "(iter)   anchor_ratios   : aspect ratios of each anchor\n",
    "(float)  iou_upper       : upper IoU used for pos boxes\n",
    "(float)  iou_lower       : lower IoU used for neg boxes\n",
    "(float)  iou_nms         : IoU used for non-max supression\n",
    "(int)    box_padding     : padding for ground-truth boxes\n",
    "(bool)   separate_maps   : if True, create parameters for each feature map separately\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create BoundingBox\n",
    "bb = BoundingBox(\n",
    "    image_shape=(240, 240),\n",
    "    classes=1,\n",
    "    c=[3, 4],\n",
    "    anchor_shapes=[32, 64],\n",
    "    anchor_scales=[0, 1, 2],\n",
    "    anchor_ratios=[0.5, 1, 2],\n",
    "    iou_upper=0.5,\n",
    "    iou_lower=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will initialze all anchors and tempalte boxes based on our specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# PRINT BOUNDING BOX SPECS\n",
    "# =============================\n",
    "\n",
    "# --- Print grid sizes\n",
    "print('---------------------------------------')\n",
    "print('Anchor Grid Sizes')\n",
    "print(bb.params['anchor_gsizes'])\n",
    "\n",
    "# --- Print template anchor box shapes\n",
    "print('---------------------------------------')\n",
    "print('Anchor Template Box Shapes')\n",
    "print(bb.params['anchor_shapes'])\n",
    "\n",
    "# --- Print anchor details\n",
    "print('---------------------------------------')\n",
    "print('Anchor scales: {}'.format(bb.params['anchor_scales']))\n",
    "print('Anchor ratios: {}'.format(bb.params['anchor_ratios']))\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('Total anchors (A) = {}'.format(\n",
    "    len(bb.params['anchor_scales']) * \n",
    "    len(bb.params['anchor_ratios'])))\n",
    "print('Total classes (k) = {}'.format(bb.params['classes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other parameters can be found in `bb.params`.\n",
    "\n",
    "**Checkpoint**: How many boxes in total are defined by the specifications above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground-truth\n",
    "\n",
    "Recall that the ground-truth predictions the box-localization CNN must produce are variable depending on the box parameterization chosen above. Predictions at *multiple feature map resolutions* must be provided for both the classification task (e.g. determine which boxes are positive) and regression task (e.g. determine what modifications are needed to template boxes to create final boxes). \n",
    "\n",
    "**Checkpoint**: How many different feature map predictions must the CNN generate in the box parameterization chosen above? What are the shapes for all predicted feature maps? Use `bb.params['inputs_shapes']` to confirm your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BoundingBox` object can create ground-truth box parameterizations using either label masks (e.g. provided in this tutorial) or boxes provided in anchor-style format e.g `[y0, x0, y1, x1]`. To generate ground-truth from a provided mask, use the `bb.convert_msk_to_box(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create box ground-truths\n",
    "box = bb.convert_msk_to_box(ys['tumor'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the parameterized box ground-truths match your expected tensor shapes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw box parameterizations are difficult to visualize and/or check. Instead to *post-process* box parameterizations, use one of the following methods:\n",
    "\n",
    "* `bb.convert_box_to_anc(...)`: method to convert box to anchors (e.g. `[y0, x0, y1, x1]`)\n",
    "* `bb.convert_box_to_msk(...)`: method to convert box to mask label for visualization\n",
    "\n",
    "For both methods, the `apply_deltas=[True/False]` flag can be used to specify whether or not to apply the box refinements (e.g. regression network predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert box to anchors\n",
    "anchors, classes = bb.convert_box_to_anc(box, apply_deltas=False)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('\\nGround-truth template boxes (before refinement):\\n')\n",
    "print(anchors)\n",
    "\n",
    "anchors, classes = bb.convert_box_to_anc(box, apply_deltas=True)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('\\nGround-truth template boxes (after refinement):\\n')\n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: why are the post-refinement boxes exactly identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert box to mask (for visualization)\n",
    "msk = bb.convert_box_to_msk(box, apply_deltas=False)\n",
    "imshow(xs['dat'][0, ..., 0], msk, title='Ground-truth template boxes (before refinement)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert box to mask (for visualization)\n",
    "msk = bb.convert_box_to_msk(box, apply_deltas=True)\n",
    "imshow(xs['dat'][0, ..., 0], msk, title='Ground-truth template boxes (after refinement)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: What happens to the appearance of boxes with variations in:\n",
    "\n",
    "* grid sizes (`c` values)\n",
    "* anchor shapes\n",
    "* anchor aspect ratios\n",
    "* anchor scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators\n",
    "\n",
    "To convert all original masks into box parameterizations, pass the existing generators into the `bb.create_generators(...)` method. This will create new generators that utilize the prior generators to load data before applying modifications needed for box parameterization (e.g. nested generators). The `msk=` argument is used to denote the key in the `ys` dictionary containing mask labels to apply box conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare generators\n",
    "gen_train, gen_valid = client.create_generators()\n",
    "gen_train, gen_valid = bb.create_generators(gen_train, gen_valid, msk='tumor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize within the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show first iteration\n",
    "xs, ys = next(gen_train)\n",
    "msk = bb.convert_box_to_msk(box=ys, apply_deltas=False)\n",
    "imshow(xs['dat'][:, 0], msk[:, 0], figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "Similar to the above, to generate modified `inputs`, pass the existing original `inputs` into the `bb.get_inputs(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create inputs\n",
    "inputs = client.get_inputs(Input)\n",
    "inputs = bb.get_inputs(inputs, Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pyramid Network\n",
    "\n",
    "Now that the inputs and outputs of the CNN have been defined, the goal is to implement a network architecture that is able to perform the desired mapping via a feature pyramid network. The contracting arm of a FPN architecture is nonspecific and can be implemented using any standard architecture.\n",
    "\n",
    "Let us define the contracting arm as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contracting arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define kwargs dictionary\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same'}\n",
    "\n",
    "# --- Define lambda functions\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.ReLU()(x)\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "\n",
    "# --- Define contracting layers\n",
    "l1 = conv1(8, inputs['dat'])\n",
    "l2 = conv1(16, conv2(16, l1))\n",
    "l3 = conv1(24, conv2(24, l2))\n",
    "l4 = conv1(32, conv2(32, l3))\n",
    "l5 = conv1(48, conv2(48, l4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: The most important part here is simply to ensure that the deepest layer is at least the same size (or smaller) than the smallest anchor grid size needed for the box network. How can we confirm this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding arm\n",
    "\n",
    "To create layers of the expanding arm of the FPN, two special new operations must be defined.\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/fpn.png)\n",
    "\n",
    "First, to upsample an FPN feature map, a simple parameterless interpolation is used. The corresponding Tensorflow class is the `layers.UpSampling3D(...)` object. Let us define the corresponding lambda function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define zoom\n",
    "zoom = lambda x : layers.UpSampling3D(\n",
    "    size=(1, 2, 2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, in order to add the corresponding contract arm layer, recall that a 1 x 1 x 1 convolution must be used to **match feature map channels (filters)**. Recall that all FPN output maps must have the same identical number of channels (in our case, 64). Let us define the corresponding lambda function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define 1 x 1 x 1 projection\n",
    "proj = lambda filters, x : layers.Conv3D(\n",
    "    filters=filters,\n",
    "    strides=1,\n",
    "    kernel_size=(1, 1, 1),\n",
    "    padding='same',\n",
    "    kernel_initializer='he_normal')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the expanding layers. Recall that we only need to create the required anchor grid sizes as defined above:\n",
    "\n",
    "* c4: 15 x 15\n",
    "* c3: 30 x 30\n",
    "\n",
    "Once these have been created, there is no need to define more expansions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define expanding layers\n",
    "l6 = proj(64, l5)\n",
    "l7 = conv1(64, zoom(l6) + proj(64, l4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNet\n",
    "\n",
    "Now that the FPN backbone has been created, we must finalize the feature maps to perform the classification and regression tasks necessary to predict boxes. There are many ways to implement this final mapping; we will derive a method based off of the approach described in the RetinaNet paper. This implementation is quite simple (most of the \"power\" lies in the focal loss function) and simply requires that at each feature map resolution, a classifier head is created to perform the necessary classification and regression tasks.\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/retinanet.png)\n",
    "\n",
    "In the original RetinaNet paper, four convolutional blocks are used, however in the context of medical imaging problems (e.g. less data, more homogenous predictions), we will just use two blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Determine filter sizes\n",
    "logits = {}\n",
    "K = 1\n",
    "A = 9\n",
    "\n",
    "# --- C3\n",
    "c3_cls = conv1(64, conv1(64, l7))\n",
    "c3_reg = conv1(64, conv1(64, l7))\n",
    "logits['cls-c3'] = layers.Conv3D(filters=(A * K), name='cls-c3', **kwargs)(c3_cls)\n",
    "logits['reg-c3'] = layers.Conv3D(filters=(A * 4), name='reg-c3', **kwargs)(c3_reg)\n",
    "\n",
    "# --- C4\n",
    "c4_cls = conv1(64, conv1(64, l6))\n",
    "c4_reg = conv1(64, conv1(64, l6))\n",
    "logits['cls-c4'] = layers.Conv3D(filters=(A * K), name='cls-c4', **kwargs)(c4_cls)\n",
    "logits['reg-c4'] = layers.Conv3D(filters=(A * 4), name='reg-c4', **kwargs)(c4_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the model can be formally created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model\n",
    "model = Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Model\n",
    "\n",
    "There are several modifications needed to compile this box model compared to the standard approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal loss\n",
    "\n",
    "The first modification is the use of a specific **focal loss**. As you recall, the **focal loss** function gradually titrates the contribution of any given prediction such that more confident correct predictions over time become weighted less than incorrect predictions.\n",
    "\n",
    "Focal loss is not a default loss function built into the standard Tensorflow 2.0 / Keras library. Accordingly, a custom function has been written for use as part of the `jarvis-md` library. It is implemented as follows:\n",
    "\n",
    "```python\n",
    "def focal_sigmoid_ce(weights=1.0, scale=1.0, gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Method to implement focal sigmoid (binary) cross-entropy loss\n",
    "\n",
    "    \"\"\"\n",
    "    def focal_sigmoid_ce(y_true, y_pred):\n",
    "\n",
    "        # --- Calculate standard cross entropy with alpha weighting\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "            labels=y_true, logits=y_pred, pos_weight=alpha)\n",
    "\n",
    "        # --- Calculate modulation to pos and neg labels \n",
    "        p = tf.math.sigmoid(y_pred)\n",
    "        modulation_pos = (1 - p) ** gamma\n",
    "        modulation_neg = p ** gamma\n",
    "\n",
    "        mask = tf.dtypes.cast(y_true, dtype=tf.bool)\n",
    "        modulation = tf.where(mask, modulation_pos, modulation_neg)\n",
    "\n",
    "        return tf.math.reduce_sum(modulation * loss * weights * scale)\n",
    "\n",
    "    return focal_sigmoid_ce\n",
    "```\n",
    "\n",
    "A custom implementation of the focal sigmoid cross-entropy loss can be invoked as follows:  \n",
    "\n",
    "```python\n",
    "# --- Create custom focal loss function\n",
    "custom.focal_sigmoid_ce()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber loss\n",
    "\n",
    "For box regression tasks, it is common to use a combination of both L1 and L2 type losses. Most commonly the desired effect is to use an L1 loss early in training (when the loss values are large) and to transition to a smoother L2 loss as the algorithm converges. One such implementation of this smooth regression loss function is the Huber loss. \n",
    "\n",
    "A custom variant of the Huber loss can be invoked as follows:\n",
    "\n",
    "```python\n",
    "# --- Create custom Huber loss function\n",
    "custom.sl1()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked loss functions\n",
    "\n",
    "As you recall, although boxes classification and regression ground truth values are calculated for **every box** per image, only a subset of boxes are used for algorithm training:\n",
    "\n",
    "* `cls` network: only boxes with IoU > 0.5 (positive) and IoU < 0.2 (negative)\n",
    "* `reg` network: only boxes that correspond to a positive classification\n",
    "\n",
    "To account for this, a mask is passed into the custom loss functions to remove the contribution of boxes that should be ignored based on the criteria above. The itself is created by the same generator that yields input data for the model.\n",
    "\n",
    "To use, simply invoke by passing the desired mask into the function initializer:\n",
    "\n",
    "```python\n",
    "# --- Create custom focal loss function\n",
    "custom.focal_sigmoid_ce(inputs['cls-c3-msk'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box metrics\n",
    "\n",
    "Finally, to keep track of classification performance, the use of the standard accuracy metric is suboptimal as the number of correct box predictions will quickly saturate to 100% (as the number of negative boxes >> number of positive boxes). In fact generally speaking, any metric that tracks performance of negative box predictions gtend not be very useful.\n",
    "\n",
    "Instead, consider the use of **sensitivity** and **PPV**:\n",
    "\n",
    "* sensitivity (recall): TP / (TP + FN)\n",
    "* positive predictive value (precision): TP / (TP + FP)\n",
    "\n",
    "A custom variant of both sensitivity and PPV metrics for binary cross-entropy loss can be invoked as follows:\n",
    "\n",
    "```python\n",
    "# --- Create sensivity and PPV metrics\n",
    "custom.sigmoid_ce_sens()\n",
    "custom.sigmoid_ce_ppv()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling\n",
    "\n",
    "Putting this all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
    "    loss={\n",
    "        'cls-c3': custom.focal_sigmoid_ce(inputs['cls-c3-msk']),\n",
    "        'cls-c4': custom.focal_sigmoid_ce(inputs['cls-c4-msk']),\n",
    "        'reg-c3': custom.sl1(inputs['reg-c3-msk']),\n",
    "        'reg-c4': custom.sl1(inputs['reg-c4-msk']),\n",
    "        },\n",
    "    metrics={\n",
    "        'cls-c3': [custom.sigmoid_ce_sens(), custom.sigmoid_ce_ppv()],\n",
    "        'cls-c4': [custom.sigmoid_ce_sens(), custom.sigmoid_ce_ppv()]},\n",
    "    experimental_run_tf_function=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-memory data\n",
    "\n",
    "For moderate sized datasets which are too large to fit into immediate hard-drive cache, but small enough to fit into RAM memory, it is often times a good idea to first load all training data into RAM memory for increased speed of training. The `client` can be used for this purpose as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data into memory for faster training\n",
    "client.load_data_in_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train model\n",
    "model.fit(\n",
    "    x=gen_train, \n",
    "    steps_per_epoch=500, \n",
    "    epochs=16,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=500,\n",
    "    validation_freq=4,\n",
    "    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To test the trained model, the following steps are required:\n",
    "\n",
    "* load data\n",
    "* use `model.predict(...)` to obtain logit scores\n",
    "* use `BoundingBox` object to convert predictions to anchors or masks\n",
    "* compare prediction with ground-truth\n",
    "* serialize in Pandas DataFrame\n",
    "\n",
    "Recall that the generator used to train the model simply iterates through the dataset randomly. For model evaluation, the cohort must instead be loaded manually in an orderly way. For this tutorial, we will create new **test mode** data generators, which will simply load each example individually once for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
    "test_train, test_valid = bb.create_generators(test_train, test_valid, msk='tumor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: although the model is trained using 2D slices, there is nothing to preclude passing an entire 3D volume through the model at one time (e.g. consider that the entire 3D volume is a single *batch* of data). In fact, typically performance metrics for medical imaging models are commonly reported on a volume-by-volume basis (not slice-by-slice). Thus, use the `expand=True` flag in `client.create_generators(...)` as above to yield entire 3D volumes instead of slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run entire volume through model\n",
    "x, y = next(test_train)\n",
    "box = model.predict(x)\n",
    "\n",
    "# --- Modification for < TF2.2\n",
    "if type(box) is list:\n",
    "    box = {k: l for k, l in zip(model.output_names, box)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating IoUs\n",
    "\n",
    "The logits are the raw predictions from the model, but to generate the corresponding boxes several post-processing steps are needed. First the positive boxes must be identified from the classification network. Then, the predicted template boxes need to be refined using the regression network:\n",
    "\n",
    "![Box Parameterization](https://raw.githubusercontent.com/peterchang77/dl_tutor/master/cs190/spring_2020/notebooks/box_localization/pngs/regression.png)\n",
    "\n",
    "The `BoundingBox` object can be used to perform these steps:\n",
    "\n",
    "* `bb.convert_box_to_msk(...)`: convert box predictions into 3D mask (primarily visualization)\n",
    "* `bb.convert_box_to_anc(...)`: convert box predictions into anchors (`[y0, x0, y1, x1]`) (calculate IoUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert to anchors\n",
    "anchors, classes = bb.convert_box_to_anc(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: what do these anchors and classes represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the ground-truth overlays at the beginning of this tutorial, a number of boxes may be classified for each single ground-truth box. Accordinging, during inference a number of ground-truth boxes may be triggered. To prune this boxes, use **non-max suppression**, a technique that removes all boxes that above a certain IoU threshold with the high scoring box. \n",
    "\n",
    "Test several different IoU thresholds to its effect on box outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show various boxes at different NMS thresholds\n",
    "msk = bb.convert_box_to_msk(box, iou_nms=0.01)\n",
    "imshow(x['dat'][0], msk[0], figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the specific prediction boxes have been isolated, compare them with the ground truth boxes using the `bb.calculate_ious(...)` method. This function will compare a given single box with a list of many ground-truth anchors. The maximum overlap generated represents the IoU value for the given prediction box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create validation generator\n",
    "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
    "test_train, test_valid = bb.create_generators(test_train, test_valid, msk='tumor')\n",
    "\n",
    "ious = {\n",
    "    'med': [],\n",
    "    'p25': [],\n",
    "    'p75': []}\n",
    "\n",
    "for x, y in test_train:\n",
    "    \n",
    "    # --- Predict\n",
    "    box = model.predict(x)\n",
    "    if type(box) is list:\n",
    "        box = {k: l for k, l in zip(model.output_names, box)}\n",
    "        \n",
    "    # --- Convert predictions to anchors\n",
    "    anchors_pred, _ = bb.convert_box_to_anc(box)\n",
    "    \n",
    "    # --- Convert ground-truth to anchors\n",
    "    anchors_true, _ = bb.convert_box_to_anc(y)\n",
    "    \n",
    "    # --- Calculate IoUs\n",
    "    curr = []\n",
    "    for pred, true in zip(anchors_pred, anchors_true):\n",
    "        for p in pred:\n",
    "            iou = bb.calculate_ious(box=p, anchors=true)\n",
    "            if iou.size > 0:\n",
    "                curr.append(np.max(iou))\n",
    "            else: \n",
    "                curr.append(0)\n",
    "    \n",
    "    if len(curr) == 0:\n",
    "        curr = [0]\n",
    "        \n",
    "    ious['med'].append(np.median(curr))\n",
    "    ious['p25'].append(np.percentile(curr, 25))\n",
    "    ious['p75'].append(np.percentile(curr, 75))\n",
    "    \n",
    "ious = {k: np.array(v) for k, v in ious.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define columns\n",
    "df = pd.DataFrame(index=np.arange(ious['med'].size))\n",
    "df['iou_median'] = ious['med']\n",
    "df['iou_p-25th'] = ious['p25']\n",
    "df['iou_p-75th'] = ious['p75']\n",
    "\n",
    "# --- Print accuracy\n",
    "print(df['iou_median'].median())\n",
    "print(df['iou_p-25th'].median())\n",
    "print(df['iou_p-75th'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "model.save('./box_localization.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "del model\n",
    "model = models.load_model('./box_localization.hdf5', compile=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
